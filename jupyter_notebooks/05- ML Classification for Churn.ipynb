{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Classification Predict Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objectives\n",
    "\n",
    "* Fit and evaluate a classification model to predict if a prospect will churn or not.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/engineered/x_train_cleaned.csv\n",
    "* outputs/datasets/engineered/x_test_cleaned.csv\n",
    "* outputs/datasets/engineered/y_train_cleaned.csv\n",
    "* outputs/datasets/engineered/y_test_cleaned.csv\n",
    "\n",
    "## Conclusions\n",
    "* Performing PCA on data delivered worse results so we refrained from using PCA.\n",
    " * For model evaluation, we used accuracy and F1-score. CatBoostClassifier model performed the best.\n",
    " * The best model achieved an accuracy of 86.08% on the test data\n",
    " * The best model achieved an F1-score of 86.29% on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since jupyter notebooks are in a subfolder we need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_folder = \"outputs/datasets/engineered\"\n",
    "\n",
    "x_train = pd.read_csv(f\"{input_folder}/x_train_cleaned.csv\")\n",
    "x_test = pd.read_csv(f\"{input_folder}/x_test_cleaned.csv\")\n",
    "y_train = pd.read_csv(f\"{input_folder}/y_train_cleaned.csv\")\n",
    "y_test = pd.read_csv(f\"{input_folder}/y_test_cleaned.csv\")\n",
    "x_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "output_folder = \"outputs/datasets/cleaned\"\n",
    "\n",
    "df = pd.read_csv(f\"{output_folder}/telco_customer_churn_cleaned.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_cat = df.select_dtypes(include=['object', 'category'])\n",
    "df_cat_vars = list(df_cat.columns) # Save list of categorical values\n",
    "\n",
    "for column in df_cat.columns:\n",
    "    df_cat[column] = le.fit_transform(df_cat[column])\n",
    "\n",
    "df_merged = df.drop(columns=df.columns.intersection(df_cat.columns)).join(df_cat)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_spearman = df_merged.corr(method='spearman')['Churn'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
    "corr_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_pearson = df_merged.corr(method='pearson')['Churn'].sort_values(key=abs, ascending=False)[1:].head(10)\n",
    "corr_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 6\n",
    "combined_list = corr_pearson[:top_n].index.to_list() + corr_spearman[:top_n].index.to_list()\n",
    "vars_to_study = list(set(combined_list))\n",
    "vars_to_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_enc_norm = pd.DataFrame(scaler.fit_transform(df_merged[vars_to_study]), columns=df_merged[vars_to_study].columns)\n",
    "df_enc_norm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_enc_norm, df_merged['Churn'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot PCA  variance cumulative graph and discover an optimal number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components that explain at least 95% of the variance\n",
    "optimal_n = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"Optimal number of components: {optimal_n}\")\n",
    "\n",
    "# Plot the explained variance\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label=\"95% Variance Threshold\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Optimal n_components for PCA\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aplly transformation with optimal number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=optimal_n)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.fit_transform(x_test)\n",
    "x_train_pca = pd.DataFrame(x_train_pca)\n",
    "x_test_pca = pd.DataFrame(x_test_pca)\n",
    "x_train_pca.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ml Classification best model and hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to look for a model with best r2 and acuraccy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  accuracy_score, f1_score\n",
    "\n",
    "\n",
    "estimators = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'SVC': SVC(random_state=42),\n",
    "    'XGBClassifier': XGBClassifier(random_state=42),\n",
    "    'CatBoostClassifier': CatBoostClassifier(random_state=42),\n",
    "    'AdaBoostClassifier': AdaBoostClassifier(random_state=42),\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier': {},\n",
    "    'DecisionTreeClassifier': {},\n",
    "    'knn': {},\n",
    "    'SVC': {},\n",
    "    'XGBClassifier': {},\n",
    "    'CatBoostClassifier': {},\n",
    "    'AdaBoostClassifier': {},\n",
    "    'GradientBoostingClassifier': {}\n",
    "}\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for key, estimator in estimators.items():\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"{key} beeing fitted\")\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=estimator, param_grid=params[key], cv=4, n_jobs=1, verbose=1, scoring='f1')\n",
    "    grid_search.fit(x_train_pca, y_train.values.ravel())\n",
    "    y_pred = grid_search.predict(x_test_pca)\n",
    "\n",
    "    results_list.append({\n",
    "        'estimator': key,\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results are below 80% and are not satisfactuary. Lets they the model search without the PCA transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for key, estimator in estimators.items():\n",
    "    \n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"{key} beeing fitted\")\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=estimator, param_grid=params[key], cv=4, n_jobs=1, verbose=1, scoring='f1')\n",
    "    grid_search.fit(x_train, y_train.values.ravel())\n",
    "    y_pred = grid_search.predict(x_test)\n",
    "\n",
    "    results_list.append({\n",
    "        'estimator': key,\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df.sort_values('f1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We get better results. We are going to perform hyperparameter search on models with best r2 score. CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimator = {\n",
    "    'CatBoostClassifier': CatBoostClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"iterations\": [100, 200, 500, 800, 1000],  # Number of boosting iterations (trees)\n",
    "    \"learning_rate\": [0.02, 0.1, 0.1, 1],  # Controls the step size of boosting\n",
    "    \"depth\": [3, 5, 10],  # Maximum depth of trees\n",
    "    \"border_count\": [32, 64, 128],  # Number of splits for numerical features\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     \"iterations\": [800],  # Number of boosting iterations (trees)\n",
    "#     \"learning_rate\": [0.02],  # Controls the step size of boosting\n",
    "#     \"depth\": [10],  # Maximum depth of trees\n",
    "#     \"border_count\": [64],  # Number of splits for numerical features\n",
    "# }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=estimator['CatBoostClassifier'], param_grid=params, cv=4, n_jobs=1, verbose=1, scoring='f1')\n",
    "grid_search.fit(x_train, y_train)\n",
    "y_pred = grid_search.predict(x_test)\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Test acuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Test F1: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now gradientBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = {\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],  # Number of boosting stages (trees)\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],  # Controls the contribution of each tree\n",
    "    \"max_depth\": [2, 3, 5],  # Depth of individual trees (controls model complexity)\n",
    "    \"subsample\": [0.7, 0.8, 1.0],  # Fraction of data used for training each tree\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples needed to split a node\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=estimator['GradientBoostingClassifier'], param_grid=params, cv=4, n_jobs=1, verbose=1, scoring='f1')\n",
    "grid_search.fit(x_train, y_train.values.ravel())\n",
    "y_pred = grid_search.predict(x_test)\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Test acuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Test F1: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will fit the model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model_optimisation(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    estimator = {\n",
    "        'CatBoostClassifier': CatBoostClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        \"iterations\": [800],  # Number of boosting iterations (trees)\n",
    "        \"learning_rate\": [0.02],  # Controls the step size of boosting\n",
    "        \"depth\": [10],  # Maximum depth of trees\n",
    "        \"border_count\": [64],  # Number of splits for numerical features\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=estimator['CatBoostClassifier'], param_grid=params, cv=4, n_jobs=1, verbose=1, scoring='f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    y_pred = grid_search.predict(x_test)\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"Test acuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"Test F1: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_fit = classification_model_optimisation(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push files to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "version = 'v1'\n",
    "file_path = f'outputs/ml_classification/predict_churn/{version}'\n",
    "\n",
    "try:\n",
    "    os.makedirs(name=file_path)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will save function with best model parameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(value=best_model_fit ,\n",
    "            filename=f\"{file_path}/ml_classification_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.to_csv(f\"{file_path}/X_train.csv\", index=False)\n",
    "y_train.to_csv(f\"{file_path}/y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.to_csv(f\"{file_path}/X_test.csv\", index=False)\n",
    "y_test.to_csv(f\"{file_path}/y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
